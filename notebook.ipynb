{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feiP6ou3TsOT",
        "outputId": "48119473-79e1-421d-f87c-59f54258fc10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Define the input video path and output directory\n",
        "video_path = '/content/drive/MyDrive/Intern_assignment/videoplayback.mp4'\n",
        "output_dir = '/content/drive/MyDrive/Intern_assignment/frames'\n",
        "num_frames_to_capture = 8\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Check if video opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video at {video_path}\")\n",
        "else:\n",
        "    # Get total number of frames in the video\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    if total_frames == 0:\n",
        "        print(\"Error: Video has no frames.\")\n",
        "    else:\n",
        "        # Generate random frame indices\n",
        "        # Ensure we don't try to capture more frames than available\n",
        "        if total_frames < num_frames_to_capture:\n",
        "            print(f\"Warning: Video has only {total_frames} frames. Capturing all available frames.\")\n",
        "            random_frame_indices = list(range(total_frames))\n",
        "        else:\n",
        "            random_frame_indices = sorted(random.sample(range(total_frames), num_frames_to_capture))\n",
        "\n",
        "        print(f\"Attempting to capture frames at indices: {random_frame_indices}\")\n",
        "\n",
        "        # Capture and save frames\n",
        "        for i, frame_idx in enumerate(random_frame_indices):\n",
        "            # Set the frame position\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "\n",
        "            # Read the frame\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if ret:\n",
        "                # Define the filename for the captured frame\n",
        "                frame_filename = os.path.join(output_dir, f\"frame_{i+1}_{frame_idx}.jpg\")\n",
        "\n",
        "                # Save the frame\n",
        "                cv2.imwrite(frame_filename, frame)\n",
        "                print(f\"Saved frame {frame_idx} as {frame_filename}\")\n",
        "            else:\n",
        "                print(f\"Warning: Could not read frame at index {frame_idx}.\")\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "    print(\"Video processing complete.\")\n",
        "\n",
        "# It's good practice to destroy any OpenCV windows, though often not visible in Colab\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tIJCjK8nTvOc",
        "outputId": "533aa0d3-ca53-4705-e605-0afd67107b73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to capture frames at indices: [47, 103, 374, 619, 640, 654, 686, 750]\n",
            "Saved frame 47 as /content/drive/MyDrive/Intern_assignment/frames/frame_1_47.jpg\n",
            "Saved frame 103 as /content/drive/MyDrive/Intern_assignment/frames/frame_2_103.jpg\n",
            "Saved frame 374 as /content/drive/MyDrive/Intern_assignment/frames/frame_3_374.jpg\n",
            "Saved frame 619 as /content/drive/MyDrive/Intern_assignment/frames/frame_4_619.jpg\n",
            "Saved frame 640 as /content/drive/MyDrive/Intern_assignment/frames/frame_5_640.jpg\n",
            "Saved frame 654 as /content/drive/MyDrive/Intern_assignment/frames/frame_6_654.jpg\n",
            "Saved frame 686 as /content/drive/MyDrive/Intern_assignment/frames/frame_7_686.jpg\n",
            "Saved frame 750 as /content/drive/MyDrive/Intern_assignment/frames/frame_8_750.jpg\n",
            "Video processing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix numpy binary-compatibility and reinstall ultralytics (run in Colab)\n",
        "import os\n",
        "import sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# 1) Uninstall problematic packages (suppress noisy output)\n",
        "print(\"Uninstalling ultralytics and numpy...\")\n",
        "!pip uninstall -y ultralytics numpy >/dev/null 2>&1 || true\n",
        "\n",
        "# 2) Install a compatible numpy then ultralytics\n",
        "print(\"Installing numpy==1.25.2 and ultralytics==8.2.103 ...\")\n",
        "!pip install --no-cache-dir numpy==1.25.2 --quiet\n",
        "!pip install --no-cache-dir ultralytics==8.2.103 --quiet\n",
        "\n",
        "# 3) Verify imports and model load\n",
        "print(\"\\nVerifying imports...\")\n",
        "import importlib, traceback\n",
        "ok = True\n",
        "try:\n",
        "    import numpy as np\n",
        "    print(\"numpy version:\", np.__version__)\n",
        "except Exception as e:\n",
        "    ok = False\n",
        "    print(\"numpy import FAILED\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "    print(\"ultralytics import: OK\")\n",
        "except Exception as e:\n",
        "    ok = False\n",
        "    print(\"ultralytics import FAILED\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "# 4) Try to load the model file if present to ensure no C-extension errors\n",
        "model_file = \"yolov8n-pose.pt\"\n",
        "if ok and os.path.exists(model_file):\n",
        "    try:\n",
        "        print(f\"Attempting to load model file {model_file} ...\")\n",
        "        _ = YOLO(model_file)\n",
        "        print(\"Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        ok = False\n",
        "        print(\"Model failed to load:\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(f\"Model file {model_file} not found locally — if not downloaded yet that's fine (Ultralytics will download when first used).\")\n",
        "\n",
        "# 5) If imports still failed, restart the runtime to ensure new binaries are picked up.\n",
        "if not ok:\n",
        "    print(\"\\nInstallation may require a runtime restart. Restarting runtime now...\")\n",
        "    # Force a runtime restart (Colab will disconnect; re-run cells after reconnect).\n",
        "    os.kill(os.getpid(), 9)\n",
        "else:\n",
        "    print(\"\\nAll checks passed. You can re-run the YOLO inference cell now.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB7T6ffCYAKt",
        "outputId": "fc193bd1-7839-4862-b3ee-b65a4aa54a23"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling ultralytics and numpy...\n",
            "Installing numpy==1.25.2 and ultralytics==8.2.103 ...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m269.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m301.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.1/875.1 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m172.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "Verifying imports...\n",
            "numpy version: 2.0.2\n",
            "ultralytics import: OK\n",
            "Attempting to load model file yolov8n-pose.pt ...\n",
            "Model loaded successfully.\n",
            "\n",
            "All checks passed. You can re-run the YOLO inference cell now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POSE ESTIMATION**"
      ],
      "metadata": {
        "id": "7TNWgdsJdJ-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run YOLOv8-Pose inference: save annotated images + keypoints JSON (robust version)\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import os, json, cv2\n",
        "\n",
        "FRAMES_DIR = \"/content/drive/MyDrive/Intern_assignment/frames\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Intern_assignment/yolo_pose_outputs\"\n",
        "ANNOTATED_DIR = os.path.join(OUTPUT_DIR, \"annotated\")\n",
        "KEYPOINTS_DIR = os.path.join(OUTPUT_DIR, \"keypoints\")\n",
        "os.makedirs(ANNOTATED_DIR, exist_ok=True)\n",
        "os.makedirs(KEYPOINTS_DIR, exist_ok=True)\n",
        "\n",
        "# Load model (uses local copy if present; otherwise ultralytics downloads it)\n",
        "model = YOLO(\"yolov8n-pose.pt\")\n",
        "\n",
        "image_paths = sorted(list(Path(FRAMES_DIR).glob(\"*.*\")))\n",
        "if len(image_paths) == 0:\n",
        "    raise FileNotFoundError(f\"No images found in {FRAMES_DIR}. Verify the path and files.\")\n",
        "\n",
        "print(f\"Running pose inference on {len(image_paths)} images...\")\n",
        "\n",
        "for img_path in image_paths:\n",
        "    img_name = img_path.name\n",
        "    # Run inference (single image)\n",
        "    res = model(str(img_path), imgsz=640)[0]  # take first Result\n",
        "\n",
        "    # Annotated image: use res.plot() which returns an RGB numpy image\n",
        "    try:\n",
        "        annotated_rgb = res.plot()\n",
        "        # Convert RGB -> BGR for cv2.imwrite\n",
        "        annotated_bgr = cv2.cvtColor(annotated_rgb, cv2.COLOR_RGB2BGR)\n",
        "        annotated_path = os.path.join(ANNOTATED_DIR, f\"annotated_{img_name}\")\n",
        "        cv2.imwrite(annotated_path, annotated_bgr)\n",
        "    except Exception as e:\n",
        "        # Fallback: just copy original if plotting fails\n",
        "        print(f\"Warning: failed to create annotated image for {img_name}: {e}\")\n",
        "        annotated_path = os.path.join(ANNOTATED_DIR, f\"annotated_{img_name}\")\n",
        "        orig = cv2.imread(str(img_path))\n",
        "        cv2.imwrite(annotated_path, orig)\n",
        "\n",
        "    # Extract keypoints\n",
        "    kp_entries = []\n",
        "    # results.keypoints may contain multiple persons; handle first person if present\n",
        "    try:\n",
        "        if hasattr(res, \"keypoints\") and len(res.keypoints) > 0:\n",
        "            # res.keypoints.xy is a list of arrays (per detected instance)\n",
        "            for inst_idx, inst_kp in enumerate(res.keypoints.xy):\n",
        "                # inst_kp: Nx2 array of [x,y] for each keypoint (or nested structure)\n",
        "                # Convert to python lists\n",
        "                kp_xy = inst_kp.tolist()\n",
        "                kp_entries.append({\"instance_index\": int(inst_idx), \"keypoints_xy\": kp_xy})\n",
        "        else:\n",
        "            kp_entries = []\n",
        "    except Exception as e:\n",
        "        print(f\"Warning extracting keypoints for {img_name}: {e}\")\n",
        "        kp_entries = []\n",
        "\n",
        "    out_json = {\n",
        "        \"image\": img_name,\n",
        "        \"num_instances\": len(kp_entries),\n",
        "        \"instances\": kp_entries,\n",
        "        # include model.names if available for keypoint label mapping (index -> label)\n",
        "        \"keypoint_labels\": getattr(model, \"names\", None)\n",
        "    }\n",
        "\n",
        "    json_path = os.path.join(KEYPOINTS_DIR, f\"{Path(img_name).stem}.json\")\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(out_json, f, indent=2)\n",
        "\n",
        "    print(f\"Processed {img_name}: annotated -> {annotated_path}, keypoints -> {json_path}\")\n",
        "\n",
        "print(\"\\nInference complete.\")\n",
        "print(\"Annotated images saved to:\", ANNOTATED_DIR)\n",
        "print(\"Keypoints JSONs saved to:\", KEYPOINTS_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uxJTap-9bYwI",
        "outputId": "d695e090-978a-4fb8-86b8-18491e1b6695"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running pose inference on 8 images...\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Intern_assignment/frames/frame_1_47.jpg: 384x640 2 persons, 14.6ms\n",
            "Speed: 1.8ms preprocess, 14.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processed frame_1_47.jpg: annotated -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/annotated/annotated_frame_1_47.jpg, keypoints -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/keypoints/frame_1_47.json\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Intern_assignment/frames/frame_2_103.jpg: 384x640 2 persons, 12.3ms\n",
            "Speed: 1.8ms preprocess, 12.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processed frame_2_103.jpg: annotated -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/annotated/annotated_frame_2_103.jpg, keypoints -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/keypoints/frame_2_103.json\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Intern_assignment/frames/frame_3_374.jpg: 384x640 2 persons, 11.8ms\n",
            "Speed: 2.0ms preprocess, 11.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processed frame_3_374.jpg: annotated -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/annotated/annotated_frame_3_374.jpg, keypoints -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/keypoints/frame_3_374.json\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Intern_assignment/frames/frame_4_619.jpg: 384x640 2 persons, 12.3ms\n",
            "Speed: 1.7ms preprocess, 12.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processed frame_4_619.jpg: annotated -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/annotated/annotated_frame_4_619.jpg, keypoints -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/keypoints/frame_4_619.json\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Intern_assignment/frames/frame_5_640.jpg: 384x640 2 persons, 9.4ms\n",
            "Speed: 2.1ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processed frame_5_640.jpg: annotated -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/annotated/annotated_frame_5_640.jpg, keypoints -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/keypoints/frame_5_640.json\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Intern_assignment/frames/frame_6_654.jpg: 384x640 2 persons, 7.2ms\n",
            "Speed: 1.3ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processed frame_6_654.jpg: annotated -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/annotated/annotated_frame_6_654.jpg, keypoints -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/keypoints/frame_6_654.json\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Intern_assignment/frames/frame_7_686.jpg: 384x640 2 persons, 7.0ms\n",
            "Speed: 1.2ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processed frame_7_686.jpg: annotated -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/annotated/annotated_frame_7_686.jpg, keypoints -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/keypoints/frame_7_686.json\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Intern_assignment/frames/frame_8_750.jpg: 384x640 2 persons, 8.0ms\n",
            "Speed: 1.7ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processed frame_8_750.jpg: annotated -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/annotated/annotated_frame_8_750.jpg, keypoints -> /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/keypoints/frame_8_750.json\n",
            "\n",
            "Inference complete.\n",
            "Annotated images saved to: /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/annotated\n",
            "Keypoints JSONs saved to: /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/keypoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANGLE ESTIMATION**"
      ],
      "metadata": {
        "id": "87sh8IQmdFa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE BLOCK 2 — Compute shoulder–elbow–wrist (arm) angles using YOLOv8-Pose keypoints,\n",
        "# save numeric results (CSV + summary TXT), and write annotated images with angles overlaid.\n",
        "#\n",
        "# Instructions:\n",
        "# 1) Run this cell in Colab after you have run the YOLOv8 inference step that created\n",
        "#    JSON keypoints in: /content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/keypoints\n",
        "# 2) This cell reads those JSONs, computes left & right elbow angles (shoulder->elbow->wrist),\n",
        "#    saves results to a CSV, saves annotated images with angle text, and writes a short summary TXT.\n",
        "# 3) If multiple person instances are present in a frame, this code uses the first detected instance.\n",
        "#\n",
        "# Output files (all saved under OUTPUT_DIR):\n",
        "#   - angles_per_frame.csv         : frame filename, left_angle_deg, right_angle_deg\n",
        "#   - annotated_angles_<frame>     : copy of annotated image with angle text overlays\n",
        "#   - arm_angle_summary.txt        : short model-choice justification, calculation notes, and geometric interpretation\n",
        "#\n",
        "# Notes on mapping: the code will try to use 'keypoint_labels' found in the JSON to locate indices.\n",
        "# If unavailable, it falls back to a default COCO-style index mapping:\n",
        "#   left_shoulder=5, right_shoulder=6, left_elbow=7, right_elbow=8, left_wrist=9, right_wrist=10\n",
        "# Adjust mapping manually in the MAPPING dictionary if your keypoint ordering differs.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import csv\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# --- Paths (adjust if necessary) ---\n",
        "KEYPOINTS_DIR = \"/content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/keypoints\"\n",
        "ANNOTATED_IN_DIR = \"/content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/annotated\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Intern_assignment/arm_angle_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "ANNOTATED_OUT = os.path.join(OUTPUT_DIR, \"annotated_with_angles\")\n",
        "os.makedirs(ANNOTATED_OUT, exist_ok=True)\n",
        "\n",
        "CSV_PATH = os.path.join(OUTPUT_DIR, \"angles_per_frame.csv\")\n",
        "SUMMARY_PATH = os.path.join(OUTPUT_DIR, \"arm_angle_summary.txt\")\n",
        "\n",
        "# --- Helper math functions ---\n",
        "def angle_between_points(a, b, c):\n",
        "    \"\"\"\n",
        "    Compute angle at point b formed by points a-b-c (i.e., shoulder->elbow->wrist).\n",
        "    Returns angle in degrees [0, 180]. a,b,c are (x,y) tuples or lists.\n",
        "    If any point is None, returns None.\n",
        "    \"\"\"\n",
        "    if a is None or b is None or c is None:\n",
        "        return None\n",
        "    ax, ay = a\n",
        "    bx, by = b\n",
        "    cx, cy = c\n",
        "    # Vectors BA and BC\n",
        "    v1 = (ax - bx, ay - by)\n",
        "    v2 = (cx - bx, cy - by)\n",
        "    # Compute dot and norms\n",
        "    dot = v1[0]*v2[0] + v1[1]*v2[1]\n",
        "    n1 = math.hypot(v1[0], v1[1])\n",
        "    n2 = math.hypot(v2[0], v2[1])\n",
        "    if n1 == 0 or n2 == 0:\n",
        "        return None\n",
        "    cos_theta = max(-1.0, min(1.0, dot / (n1 * n2)))\n",
        "    theta = math.degrees(math.acos(cos_theta))\n",
        "    return theta\n",
        "\n",
        "# --- Attempt to read all JSON keypoint files ---\n",
        "kp_files = sorted(list(Path(KEYPOINTS_DIR).glob(\"*.json\")))\n",
        "if len(kp_files) == 0:\n",
        "    raise FileNotFoundError(f\"No keypoint JSON files found in {KEYPOINTS_DIR}. Run YOLOv8-Pose inference first.\")\n",
        "\n",
        "results_rows = []\n",
        "frames_used = []\n",
        "\n",
        "# Default fallback mapping (COCO-like). Modify if your JSON reveals a different order.\n",
        "FALLBACK_MAPPING = {\n",
        "    \"left_shoulder\": 5,\n",
        "    \"right_shoulder\": 6,\n",
        "    \"left_elbow\": 7,\n",
        "    \"right_elbow\": 8,\n",
        "    \"left_wrist\": 9,\n",
        "    \"right_wrist\": 10\n",
        "}\n",
        "\n",
        "# Process each keypoint JSON\n",
        "for kp_file in kp_files:\n",
        "    with open(kp_file, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    image_name = data.get(\"image\", Path(kp_file).stem)\n",
        "    frames_used.append(image_name)\n",
        "\n",
        "    # Attempt to pick the first instance (if multiple persons detected)\n",
        "    instances = data.get(\"instances\", [])\n",
        "    if len(instances) == 0:\n",
        "        # No keypoints - record Nones\n",
        "        left_angle = None\n",
        "        right_angle = None\n",
        "        results_rows.append((image_name, left_angle, right_angle))\n",
        "        continue\n",
        "\n",
        "    inst = instances[0]  # first detected person\n",
        "    kp_xy = inst.get(\"keypoints_xy\", None)\n",
        "    if kp_xy is None:\n",
        "        # legacy structure support: maybe data has keypoints_xy at root\n",
        "        kp_xy = data.get(\"keypoints_xy\", None)\n",
        "\n",
        "    if kp_xy is None:\n",
        "        left_angle = None\n",
        "        right_angle = None\n",
        "        results_rows.append((image_name, left_angle, right_angle))\n",
        "        continue\n",
        "\n",
        "    # Determine mapping from labels if available\n",
        "    mapping = {}\n",
        "    labels = data.get(\"keypoint_labels\", None)\n",
        "    if labels and isinstance(labels, (list, dict)):\n",
        "        # labels can be a list (index -> label) or dict; normalize to list if possible\n",
        "        if isinstance(labels, dict):\n",
        "            # dict mapping; try to invert if needed\n",
        "            label_list = [labels.get(str(i)) for i in range(len(kp_xy))] if len(labels) >= len(kp_xy) else None\n",
        "        else:\n",
        "            label_list = labels\n",
        "        # Search for common names\n",
        "        if label_list:\n",
        "            for i, lab in enumerate(label_list):\n",
        "                lname = str(lab).lower()\n",
        "                if \"left\" in lname and (\"shoulder\" in lname or \"sho\" in lname):\n",
        "                    mapping[\"left_shoulder\"] = i\n",
        "                if \"right\" in lname and (\"shoulder\" in lname or \"sho\" in lname):\n",
        "                    mapping[\"right_shoulder\"] = i\n",
        "                if \"left\" in lname and (\"elbow\" in lname):\n",
        "                    mapping[\"left_elbow\"] = i\n",
        "                if \"right\" in lname and (\"elbow\" in lname):\n",
        "                    mapping[\"right_elbow\"] = i\n",
        "                if \"left\" in lname and (\"wrist\" in lname):\n",
        "                    mapping[\"left_wrist\"] = i\n",
        "                if \"right\" in lname and (\"wrist\" in lname):\n",
        "                    mapping[\"right_wrist\"] = i\n",
        "\n",
        "    # Fill missing mapping fields with fallback indices\n",
        "    for k, fallback_idx in FALLBACK_MAPPING.items():\n",
        "        if k not in mapping:\n",
        "            mapping[k] = fallback_idx\n",
        "\n",
        "    # Safely index into kp_xy (guard against out-of-range)\n",
        "    def safe_get(idx):\n",
        "        try:\n",
        "            pt = kp_xy[idx]\n",
        "            if pt is None:\n",
        "                return None\n",
        "            # Some outputs may contain [x, y, conf]; handle that\n",
        "            if len(pt) >= 2:\n",
        "                return (float(pt[0]), float(pt[1]))\n",
        "        except Exception:\n",
        "            return None\n",
        "        return None\n",
        "\n",
        "    left_sh = safe_get(mapping[\"left_shoulder\"])\n",
        "    left_el = safe_get(mapping[\"left_elbow\"])\n",
        "    left_wr = safe_get(mapping[\"left_wrist\"])\n",
        "    right_sh = safe_get(mapping[\"right_shoulder\"])\n",
        "    right_el = safe_get(mapping[\"right_elbow\"])\n",
        "    right_wr = safe_get(mapping[\"right_wrist\"])\n",
        "\n",
        "    left_angle = angle_between_points(left_sh, left_el, left_wr)\n",
        "    right_angle = angle_between_points(right_sh, right_el, right_wr)\n",
        "\n",
        "    results_rows.append((image_name, left_angle, right_angle))\n",
        "\n",
        "    # Annotate existing annotated image (if available) with angle text and save to OUTPUT folder\n",
        "    annotated_in_path = os.path.join(ANNOTATED_IN_DIR, image_name)\n",
        "    if not os.path.exists(annotated_in_path):\n",
        "        # try common prefix \"annotated_\"\n",
        "        alt_path = os.path.join(ANNOTATED_IN_DIR, f\"annotated_{image_name}\")\n",
        "        if os.path.exists(alt_path):\n",
        "            annotated_in_path = alt_path\n",
        "\n",
        "    if os.path.exists(annotated_in_path):\n",
        "        im = cv2.imread(annotated_in_path)\n",
        "        if im is None:\n",
        "            # skip annotation if image can't be read\n",
        "            continue\n",
        "        # Overlay left/right angle near elbows (if available)\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        if left_el is not None and left_angle is not None:\n",
        "            lx, ly = int(left_el[0]), int(left_el[1])\n",
        "            text = f\"L:{left_angle:.1f}deg\"\n",
        "            cv2.putText(im, text, (lx+5, ly-10), font, 0.6, (0,255,0), 2, cv2.LINE_AA)\n",
        "        if right_el is not None and right_angle is not None:\n",
        "            rx, ry = int(right_el[0]), int(right_el[1])\n",
        "            text = f\"R:{right_angle:.1f}deg\"\n",
        "            cv2.putText(im, text, (rx+5, ry-10), font, 0.6, (0,255,0), 2, cv2.LINE_AA)\n",
        "\n",
        "        out_ann_path = os.path.join(ANNOTATED_OUT, f\"angles_{image_name}\")\n",
        "        cv2.imwrite(out_ann_path, im)\n",
        "\n",
        "# --- Write CSV of results ---\n",
        "with open(CSV_PATH, \"w\", newline=\"\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"frame\", \"left_arm_angle_deg\", \"right_arm_angle_deg\"])\n",
        "    for row in results_rows:\n",
        "        # format None as empty string\n",
        "        writer.writerow([row[0],\n",
        "                         \"\" if row[1] is None else f\"{row[1]:.3f}\",\n",
        "                         \"\" if row[2] is None else f\"{row[2]:.3f}\"])\n",
        "\n",
        "# --- Prepare short summary text (model-choice justification + calculation + interpretation) ---\n",
        "# Model-choice justification (1-2 sentences)\n",
        "model_choice_justification = (\n",
        "    \"Model choice: YOLOv8-Pose (ultralytics). Chosen because it installs and runs reliably in this Python/Colab \"\n",
        "    \"environment, downloads a small pose model automatically, and provides direct keypoint outputs suitable for \"\n",
        "    \"computing joint angles quickly.\"\n",
        ")\n",
        "\n",
        "# Calculation explanation (2-3 sentences)\n",
        "calc_explanation = (\n",
        "    \"Calculation: the elbow angle is computed at the elbow landmark using the three 2D points \"\n",
        "    \"(shoulder -> elbow -> wrist). The angle is the arccosine of the dot product between vectors (shoulder - elbow) \"\n",
        "    \"and (wrist - elbow), converted to degrees.\"\n",
        ")\n",
        "\n",
        "# Simple geometric interpretation: compute deltas between first and last frames when possible\n",
        "interpret_lines = []\n",
        "# gather numeric arrays for left & right\n",
        "left_vals = [r[1] for r in results_rows if r[1] is not None]\n",
        "right_vals = [r[2] for r in results_rows if r[2] is not None]\n",
        "if len(left_vals) >= 2:\n",
        "    left_delta = left_vals[-1] - left_vals[0]\n",
        "    interpret_lines.append(f\"Left elbow angle changed by {left_delta:.1f}° between first and last measured frames.\")\n",
        "elif len(left_vals) == 1:\n",
        "    interpret_lines.append(\"Left elbow angle measured in one frame only — no across-frame change computed.\")\n",
        "else:\n",
        "    interpret_lines.append(\"Left elbow angle could not be measured in any frames.\")\n",
        "\n",
        "if len(right_vals) >= 2:\n",
        "    right_delta = right_vals[-1] - right_vals[0]\n",
        "    interpret_lines.append(f\"Right elbow angle changed by {right_delta:.1f}° between first and last measured frames.\")\n",
        "elif len(right_vals) == 1:\n",
        "    interpret_lines.append(\"Right elbow angle measured in one frame only — no across-frame change computed.\")\n",
        "else:\n",
        "    interpret_lines.append(\"Right elbow angle could not be measured in any frames.\")\n",
        "\n",
        "# Detailed per-frame geometric interpret summary (2-3 lines)\n",
        "per_frame_lines = []\n",
        "for img, la, ra in results_rows:\n",
        "    la_str = \"NA\" if la is None else f\"{la:.1f}°\"\n",
        "    ra_str = \"NA\" if ra is None else f\"{ra:.1f}°\"\n",
        "    per_frame_lines.append(f\"{img}: Left={la_str}, Right={ra_str}\")\n",
        "\n",
        "with open(SUMMARY_PATH, \"w\") as sf:\n",
        "    sf.write(\"=== Model choice justification ===\\n\")\n",
        "    sf.write(model_choice_justification + \"\\n\\n\")\n",
        "    sf.write(\"=== Calculation explanation ===\\n\")\n",
        "    sf.write(calc_explanation + \"\\n\\n\")\n",
        "    sf.write(\"=== Geometric interpretation (summary) ===\\n\")\n",
        "    for line in interpret_lines:\n",
        "        sf.write(line + \"\\n\")\n",
        "    sf.write(\"\\n=== Per-frame angles ===\\n\")\n",
        "    for line in per_frame_lines:\n",
        "        sf.write(line + \"\\n\")\n",
        "\n",
        "# --- Print concise results to notebook output for quick inspection ---\n",
        "print(\"Frames used (in order):\")\n",
        "for f in frames_used:\n",
        "    print(\" -\", f)\n",
        "\n",
        "print(\"\\nComputed angles (Left, Right) in degrees:\")\n",
        "for img, la, ra in results_rows:\n",
        "    la_s = \"NA\" if la is None else f\"{la:.1f}\"\n",
        "    ra_s = \"NA\" if ra is None else f\"{ra:.1f}\"\n",
        "    print(f\"{img}: Left={la_s}°, Right={ra_s}°\")\n",
        "\n",
        "print(f\"\\nCSV saved to: {CSV_PATH}\")\n",
        "print(f\"Annotated images with angles saved to: {ANNOTATED_OUT}\")\n",
        "print(f\"Text summary (justification + calculation + interpretation) saved to: {SUMMARY_PATH}\")\n",
        "\n",
        "# End of CODE BLOCK 2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEz8klsvcqPe",
        "outputId": "06863396-21db-43d7-e298-af7fc78ec096"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frames used (in order):\n",
            " - frame_1_47.jpg\n",
            " - frame_2_103.jpg\n",
            " - frame_3_374.jpg\n",
            " - frame_4_619.jpg\n",
            " - frame_5_640.jpg\n",
            " - frame_6_654.jpg\n",
            " - frame_7_686.jpg\n",
            " - frame_8_750.jpg\n",
            "\n",
            "Computed angles (Left, Right) in degrees:\n",
            "frame_1_47.jpg: Left=155.2°, Right=156.6°\n",
            "frame_2_103.jpg: Left=150.4°, Right=169.5°\n",
            "frame_3_374.jpg: Left=7.3°, Right=73.7°\n",
            "frame_4_619.jpg: Left=116.6°, Right=142.8°\n",
            "frame_5_640.jpg: Left=103.8°, Right=131.2°\n",
            "frame_6_654.jpg: Left=95.1°, Right=134.7°\n",
            "frame_7_686.jpg: Left=64.3°, Right=102.4°\n",
            "frame_8_750.jpg: Left=67.1°, Right=150.7°\n",
            "\n",
            "CSV saved to: /content/drive/MyDrive/Intern_assignment/arm_angle_outputs/angles_per_frame.csv\n",
            "Annotated images with angles saved to: /content/drive/MyDrive/Intern_assignment/arm_angle_outputs/annotated_with_angles\n",
            "Text summary (justification + calculation + interpretation) saved to: /content/drive/MyDrive/Intern_assignment/arm_angle_outputs/arm_angle_summary.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FRAME DESCRIPTION**"
      ],
      "metadata": {
        "id": "iBniRo1smVOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate one-sentence pose estimates per frame and save to a text file in a separate folder.\n",
        "# - Uses existing YOLOv8-Pose keypoints JSONs if present (faster).\n",
        "# - Otherwise runs YOLOv8-pose on each image to obtain keypoints.\n",
        "# - Produces one concise, exam-style sentence per frame describing pose (elbow angles, arms up/down, shoulder tilt).\n",
        "# - Saves the text file to: /content/drive/MyDrive/Intern_assignment/pose_estimates/pose_descriptions.txt\n",
        "#\n",
        "# Run this in Colab after mounting Drive. Assumes ultralytics is installed and workable.\n",
        "# If ultralytics import fails, the script will try only to use existing JSONs and will error if none exist.\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import math\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# --- User paths ---\n",
        "FRAMES_DIR = Path(\"/content/drive/MyDrive/Intern_assignment/frames\")\n",
        "YOLO_KEYPOINTS_DIR = Path(\"/content/drive/MyDrive/Intern_assignment/yolo_pose_outputs/keypoints\")\n",
        "OUTPUT_TEXT_DIR = Path(\"/content/drive/MyDrive/Intern_assignment/pose_estimates\")\n",
        "OUTPUT_TEXT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUT_TEXT_PATH = OUTPUT_TEXT_DIR / \"pose_descriptions.txt\"\n",
        "\n",
        "# --- Helper math functions ---\n",
        "def angle_between(a, b, c):\n",
        "    \"\"\"Angle at b formed by a-b-c in degrees. Returns float or None.\"\"\"\n",
        "    if a is None or b is None or c is None:\n",
        "        return None\n",
        "    ax, ay = a; bx, by = b; cx, cy = c\n",
        "    v1 = (ax - bx, ay - by)\n",
        "    v2 = (cx - bx, cy - by)\n",
        "    dot = v1[0]*v2[0] + v1[1]*v2[1]\n",
        "    n1 = math.hypot(*v1)\n",
        "    n2 = math.hypot(*v2)\n",
        "    if n1 == 0 or n2 == 0:\n",
        "        return None\n",
        "    cosv = max(-1.0, min(1.0, dot / (n1*n2)))\n",
        "    return math.degrees(math.acos(cosv))\n",
        "\n",
        "def safe_get_point(kp_list, idx):\n",
        "    \"\"\"Return (x,y) or None if unavailable.\"\"\"\n",
        "    try:\n",
        "        pt = kp_list[idx]\n",
        "        if pt is None:\n",
        "            return None\n",
        "        if len(pt) >= 2:\n",
        "            return (float(pt[0]), float(pt[1]))\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "# --- Common label order used by ultralytics/yolov8-pose (fallback) ---\n",
        "DEFAULT_LABELS = [\n",
        "    \"nose\",\"left_eye\",\"right_eye\",\"left_ear\",\"right_ear\",\n",
        "    \"left_shoulder\",\"right_shoulder\",\"left_elbow\",\"right_elbow\",\n",
        "    \"left_wrist\",\"right_wrist\",\"left_hip\",\"right_hip\",\n",
        "    \"left_knee\",\"right_knee\",\"left_ankle\",\"right_ankle\"\n",
        "]\n",
        "label_to_idx_default = {lab: i for i, lab in enumerate(DEFAULT_LABELS)}\n",
        "\n",
        "# --- Load existing keypoints JSONs (if available) ---\n",
        "def load_keypoints_jsons(kp_dir):\n",
        "    files = sorted(kp_dir.glob(\"*.json\"))\n",
        "    kp_map = {}\n",
        "    for jf in files:\n",
        "        try:\n",
        "            with open(jf, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "            fname = data.get(\"image\", jf.stem)\n",
        "            kp_map[fname] = data\n",
        "        except Exception:\n",
        "            continue\n",
        "    return kp_map\n",
        "\n",
        "existing_kp = {}\n",
        "if YOLO_KEYPOINTS_DIR.exists():\n",
        "    existing_kp = load_keypoints_jsons(YOLO_KEYPOINTS_DIR)\n",
        "\n",
        "# --- Prepare list of frame file names (preserve order) ---\n",
        "img_paths = sorted([p for p in FRAMES_DIR.glob(\"*\") if p.suffix.lower() in [\".jpg\",\".jpeg\",\".png\",\".bmp\"]])\n",
        "if not img_paths:\n",
        "    raise FileNotFoundError(f\"No images found in {FRAMES_DIR}\")\n",
        "\n",
        "# --- If no existing keypoints, prepare to run YOLOv8-pose ---\n",
        "need_model = False\n",
        "for p in img_paths:\n",
        "    if p.name not in existing_kp:\n",
        "        need_model = True\n",
        "        break\n",
        "\n",
        "model = None\n",
        "if need_model:\n",
        "    try:\n",
        "        model = YOLO(\"yolov8n-pose.pt\")  # will download model if missing\n",
        "    except Exception as e:\n",
        "        # If model load fails and there are no keypoints at all, we cannot continue.\n",
        "        if not existing_kp:\n",
        "            raise RuntimeError(\"YOLOv8 model could not be loaded and no existing keypoints found. Error: \" + str(e))\n",
        "        model = None\n",
        "\n",
        "# --- Function to obtain keypoints list for a frame (first instance) ---\n",
        "def get_keypoints_for_image(img_path, existing_map, model):\n",
        "    \"\"\"Returns (image_name, width, height, kp_xy_list, keypoint_labels)\"\"\"\n",
        "    img_name = img_path.name\n",
        "    # Priority: existing JSON\n",
        "    if img_name in existing_map:\n",
        "        data = existing_map[img_name]\n",
        "        instances = data.get(\"instances\", [])\n",
        "        if instances:\n",
        "            kpxy = instances[0].get(\"keypoints_xy\", [])\n",
        "        else:\n",
        "            kpxy = []\n",
        "        w = data.get(\"width\", None)\n",
        "        h = data.get(\"height\", None)\n",
        "        labels = data.get(\"keypoint_labels\", None)\n",
        "        return img_name, w, h, kpxy, labels\n",
        "    # Else run model inference\n",
        "    if model is None:\n",
        "        return img_name, None, None, [], None\n",
        "    res = model(str(img_path), imgsz=640)[0]\n",
        "    # try to get image size\n",
        "    img = cv2.imread(str(img_path))\n",
        "    h, w = (img.shape[0], img.shape[1]) if img is not None else (None, None)\n",
        "    kpxy = []\n",
        "    labels = getattr(model, \"names\", None)\n",
        "    try:\n",
        "        if hasattr(res, \"keypoints\") and len(res.keypoints) > 0:\n",
        "            # take first instance\n",
        "            kpxy = res.keypoints.xy[0].tolist()\n",
        "        else:\n",
        "            kpxy = []\n",
        "    except Exception:\n",
        "        kpxy = []\n",
        "    return img_name, w, h, kpxy, labels\n",
        "\n",
        "# --- Compose one-sentence description for single frame ---\n",
        "def describe_pose_single(img_name, w, h, kpxy, labels):\n",
        "    \"\"\"Return one concise sentence describing pose for this frame.\"\"\"\n",
        "    # Build index map\n",
        "    idx_map = {}\n",
        "    if labels:\n",
        "        # labels may be a dict or list\n",
        "        if isinstance(labels, dict):\n",
        "            # try to build list by numeric keys\n",
        "            try:\n",
        "                label_list = [labels.get(str(i)) for i in range(len(kpxy))]\n",
        "            except Exception:\n",
        "                label_list = None\n",
        "        else:\n",
        "            label_list = labels\n",
        "        if isinstance(label_list, list) and all(label_list):\n",
        "            idx_map = {str(l).lower(): i for i, l in enumerate(label_list)}\n",
        "    if not idx_map:\n",
        "        idx_map = {k: v for k, v in label_to_idx_default.items()}\n",
        "\n",
        "    def pt(name):\n",
        "        ix = idx_map.get(name.lower())\n",
        "        if ix is None:\n",
        "            return None\n",
        "        return safe_get_point(kpxy, ix)\n",
        "\n",
        "    nose = pt(\"nose\")\n",
        "    l_sh = pt(\"left_shoulder\"); r_sh = pt(\"right_shoulder\")\n",
        "    l_el = pt(\"left_elbow\"); r_el = pt(\"right_elbow\")\n",
        "    l_wr = pt(\"left_wrist\"); r_wr = pt(\"right_wrist\")\n",
        "    l_hip = pt(\"left_hip\"); r_hip = pt(\"right_hip\")\n",
        "\n",
        "    # numeric features\n",
        "    left_elbow_ang = angle_between(l_sh, l_el, l_wr)\n",
        "    right_elbow_ang = angle_between(r_sh, r_el, r_wr)\n",
        "\n",
        "    # wrists relative to shoulders (vertical)\n",
        "    def wrist_vs_sh(wr, sh):\n",
        "        if wr is None or sh is None or h is None:\n",
        "            return None\n",
        "        # Negative means wrist is above shoulder (arm raised)\n",
        "        return float(wr[1]) - float(sh[1])\n",
        "\n",
        "    l_wrs = wrist_vs_sh(l_wr, l_sh)\n",
        "    r_wrs = wrist_vs_sh(r_wr, r_sh)\n",
        "\n",
        "    # Shoulder tilt angle (left->right)\n",
        "    def shoulder_tilt_deg(ls, rs):\n",
        "        if ls is None or rs is None:\n",
        "            return None\n",
        "        dx = rs[0] - ls[0]\n",
        "        dy = rs[1] - ls[1]\n",
        "        return math.degrees(math.atan2(dy, dx))\n",
        "\n",
        "    sh_tilt = shoulder_tilt_deg(l_sh, r_sh)\n",
        "\n",
        "    # Compose description parts\n",
        "    parts = []\n",
        "    if left_elbow_ang is not None:\n",
        "        parts.append(f\"left elbow ~{left_elbow_ang:.0f}°\")\n",
        "    if right_elbow_ang is not None:\n",
        "        parts.append(f\"right elbow ~{right_elbow_ang:.0f}°\")\n",
        "\n",
        "    # arms up/down detection using image height scale\n",
        "    arm_note = None\n",
        "    if h:\n",
        "        thr = 0.03 * h  # 3% image height\n",
        "        left_pos = None\n",
        "        right_pos = None\n",
        "        if l_wrs is not None:\n",
        "            left_pos = \"raised\" if l_wrs < -thr else (\"lowered\" if l_wrs > thr else \"neutral\")\n",
        "        if r_wrs is not None:\n",
        "            right_pos = \"raised\" if r_wrs < -thr else (\"lowered\" if r_wrs > thr else \"neutral\")\n",
        "        if left_pos and right_pos:\n",
        "            if left_pos == right_pos:\n",
        "                arm_note = f\"both arms {left_pos}\" if left_pos != \"neutral\" else None\n",
        "            else:\n",
        "                # prefer naming which arm is raised/lowered\n",
        "                notes = []\n",
        "                if left_pos != \"neutral\":\n",
        "                    notes.append(f\"left arm {left_pos}\")\n",
        "                if right_pos != \"neutral\":\n",
        "                    notes.append(f\"right arm {right_pos}\")\n",
        "                arm_note = \", \".join(notes)\n",
        "        elif left_pos:\n",
        "            arm_note = f\"left arm {left_pos}\" if left_pos != \"neutral\" else None\n",
        "        elif right_pos:\n",
        "            arm_note = f\"right arm {right_pos}\" if right_pos != \"neutral\" else None\n",
        "    if arm_note:\n",
        "        parts.append(arm_note)\n",
        "\n",
        "    if sh_tilt is not None:\n",
        "        # small rounding, indicate tilt direction\n",
        "        direction = \"tilt clockwise\" if sh_tilt > 6 else (\"tilt counterclockwise\" if sh_tilt < -6 else None)\n",
        "        if direction:\n",
        "            parts.append(f\"shoulder {direction} {sh_tilt:.1f}°\")\n",
        "\n",
        "    # If no meaningful parts, say pose detected but incomplete\n",
        "    if not parts:\n",
        "        sentence = f\"{img_name}: Pose detected but keypoint estimates incomplete.\"\n",
        "    else:\n",
        "        sentence = f\"{img_name}: \" + \"; \".join(parts) + \".\"\n",
        "\n",
        "    return sentence\n",
        "\n",
        "# --- Main loop: build descriptions for all frames, save text file ---\n",
        "lines = []\n",
        "for img_path in img_paths:\n",
        "    name, w, h, kpxy, labels = get_keypoints_for_image(img_path, existing_kp, model)\n",
        "    # If kpxy is empty, attempt to run model again (robustness)\n",
        "    if (not kpxy) and model is not None:\n",
        "        try:\n",
        "            name, w, h, kpxy, labels = get_keypoints_for_image(img_path, existing_kp, model)\n",
        "        except Exception:\n",
        "            pass\n",
        "    desc = describe_pose_single(name, w, h, kpxy, labels)\n",
        "    lines.append(desc)\n",
        "\n",
        "# Save to text file\n",
        "with open(OUT_TEXT_PATH, \"w\") as f:\n",
        "    for line in lines:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# Also print brief confirmation + first few lines\n",
        "print(f\"Wrote {len(lines)} pose descriptions to: {OUT_TEXT_PATH}\\n\")\n",
        "for l in lines[:min(8, len(lines))]:\n",
        "    print(l)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpGc1tv9dOmN",
        "outputId": "8bd728cb-b905-4f24-ab6d-6bc43ae113e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote 8 pose descriptions to: /content/drive/MyDrive/Intern_assignment/pose_estimates/pose_descriptions.txt\n",
            "\n",
            "frame_1_47.jpg: left elbow ~155°; right elbow ~157°; shoulder tilt clockwise 175.4°.\n",
            "frame_2_103.jpg: left elbow ~150°; right elbow ~170°; shoulder tilt clockwise 177.4°.\n",
            "frame_3_374.jpg: left elbow ~7°; right elbow ~74°; shoulder tilt counterclockwise -151.7°.\n",
            "frame_4_619.jpg: left elbow ~117°; right elbow ~143°; shoulder tilt clockwise 11.7°.\n",
            "frame_5_640.jpg: left elbow ~104°; right elbow ~131°; shoulder tilt clockwise 13.5°.\n",
            "frame_6_654.jpg: left elbow ~95°; right elbow ~135°; shoulder tilt clockwise 8.2°.\n",
            "frame_7_686.jpg: left elbow ~64°; right elbow ~102°; shoulder tilt clockwise 11.8°.\n",
            "frame_8_750.jpg: left elbow ~67°; right elbow ~151°.\n"
          ]
        }
      ]
    }
  ]
}